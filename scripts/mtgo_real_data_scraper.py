#!/usr/bin/env python3
"""
Scraper MTGO Donn√©es R√©elles - R√©cup√®re les vraies donn√©es avec la nouvelle structure

R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT
- Tous les fichiers existants doivent √™tre pr√©serv√©s
- Seulement AJOUTER de nouvelles donn√©es
- Aucune suppression, remplacement ou √©crasement autoris√©

Analyse de la nouvelle structure MTGO :
- Les pages principales sont des pages de pr√©sentation
- Les vraies donn√©es sont dans des URLs sp√©cifiques
- Recherche des vraies URLs de donn√©es
"""

import json
import logging
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

import requests
from bs4 import BeautifulSoup

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class MTGORealDataScraper:
    """Scraper qui trouve les vraies donn√©es MTGO"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )
        self.cache_dir = Path("data/raw/mtgo")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

        # R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE
        self.logger.info("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
        self.logger.info("üìã Seulement AJOUTER de nouvelles donn√©es")

        # URLs √† explorer pour trouver les vraies donn√©es
        self.exploration_urls = [
            "https://www.mtgo.com",
            "https://www.mtgo.com/en/mtgo",
            "https://www.mtgo.com/en/mtgo/tournaments",
            "https://www.mtgo.com/en/mtgo/decklists",
            "https://www.mtgo.com/en/mtgo/results",
            "https://www.mtgo.com/en/mtgo/standings",
            "https://www.mtgo.com/en/mtgo/news",
            "https://www.mtgo.com/en/mtgo/events",
        ]

    def find_real_data_urls(self):
        """Trouve les vraies URLs de donn√©es MTGO"""
        self.logger.info("üîç Recherche des vraies URLs de donn√©es MTGO")

        all_links = []

        for url in self.exploration_urls:
            try:
                self.logger.info(f"üîç Exploration: {url}")

                response = self.session.get(url, timeout=30)
                if response.status_code == 200:
                    links = self.extract_potential_data_links(response.text, url)
                    all_links.extend(links)
                    self.logger.info(
                        f"‚úÖ {len(links)} liens potentiels trouv√©s sur {url}"
                    )
                else:
                    self.logger.warning(f"‚ùå Erreur {response.status_code} pour {url}")

                time.sleep(2)

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur avec {url}: {e}")

        # Filtrer les liens pertinents
        data_links = self.filter_data_links(all_links)

        self.logger.info(f"üéØ {len(data_links)} URLs de donn√©es identifi√©es")
        return data_links

    def extract_potential_data_links(self, html_content: str, source_url: str) -> list:
        """Extrait tous les liens potentiels d'une page"""
        links = []
        soup = BeautifulSoup(html_content, "html.parser")

        # Chercher tous les liens
        all_links = soup.find_all("a", href=True)

        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True)

            # Construire l'URL compl√®te
            if href.startswith("/"):
                full_url = f"https://www.mtgo.com{href}"
            elif href.startswith("http"):
                full_url = href
            else:
                full_url = f"https://www.mtgo.com/{href}"

            links.append({"url": full_url, "text": text, "source": source_url})

        return links

    def filter_data_links(self, all_links: list) -> list:
        """Filtre les liens pour ne garder que ceux qui contiennent des donn√©es"""
        data_links = []

        # Mots-cl√©s pour identifier les liens de donn√©es
        data_keywords = [
            "decklist",
            "tournament",
            "challenge",
            "league",
            "preliminary",
            "showcase",
            "qualifier",
            "result",
            "standing",
            "ranking",
            "modern",
            "standard",
            "legacy",
            "pioneer",
            "vintage",
            "pauper",
            "event",
            "competition",
            "match",
            "game",
            "player",
            "winner",
        ]

        for link_info in all_links:
            url_lower = link_info["url"].lower()
            text_lower = link_info["text"].lower()

            # V√©rifier si le lien contient des mots-cl√©s de donn√©es
            if any(
                keyword in url_lower or keyword in text_lower
                for keyword in data_keywords
            ):
                # √âviter les liens de navigation g√©n√©riques
                if not any(
                    exclude in url_lower
                    for exclude in ["#", "javascript:", "mailto:", "tel:"]
                ):
                    data_links.append(link_info)

        return data_links

    def scrape_real_data(self, data_links: list):
        """Scrape les vraies donn√©es depuis les URLs identifi√©es"""
        self.logger.info("üöÄ D√©but du scraping des vraies donn√©es MTGO")

        # V√©rifier le cache existant AVANT de commencer
        existing_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(f"üìÅ Cache existant : {len(existing_files)} fichiers pr√©serv√©s")

        all_data = []

        for link_info in data_links:
            try:
                self.logger.info(
                    f"üì° Scraping: {link_info['text']} -> {link_info['url']}"
                )

                response = self.session.get(link_info["url"], timeout=30)
                if response.status_code == 200:
                    data = self.extract_data_from_page(response.text, link_info)
                    all_data.extend(data)
                    self.logger.info(
                        f"‚úÖ {len(data)} √©l√©ments trouv√©s sur {link_info['url']}"
                    )
                else:
                    self.logger.warning(
                        f"‚ùå Erreur {response.status_code} pour {link_info['url']}"
                    )

                time.sleep(2)  # Pause pour √©viter d'√™tre bloqu√©

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur avec {link_info['url']}: {e}")

        # Sauvegarder toutes les donn√©es
        if all_data:
            self.save_real_data(all_data)
        else:
            self.logger.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e")

        # V√©rifier le cache APR√àS sauvegarde
        final_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(
            f"üìÅ Cache final : {len(final_files)} fichiers (pr√©server + ajout)"
        )
        self.logger.info(
            f"‚úÖ R√àGLE RESPECT√âE : {len(final_files) - len(existing_files)} nouveaux fichiers ajout√©s"
        )

    def extract_data_from_page(self, html_content: str, link_info: dict) -> list:
        """Extrait les donn√©es d'une page sp√©cifique"""
        data = []
        soup = BeautifulSoup(html_content, "html.parser")

        # Chercher tous les √©l√©ments de contenu
        content_elements = soup.find_all(
            ["div", "article", "section", "p", "h1", "h2", "h3", "h4", "h5", "h6"]
        )

        for element in content_elements:
            text = element.get_text(strip=True)

            # V√©rifier si le contenu contient des informations MTG
            if self.is_mtg_content(text):
                item_data = self.create_data_item(element, text, link_info)
                if item_data:
                    data.append(item_data)

        return data

    def is_mtg_content(self, text: str) -> bool:
        """V√©rifie si le texte contient des informations MTG"""
        mtg_keywords = [
            "modern",
            "standard",
            "legacy",
            "pioneer",
            "vintage",
            "pauper",
            "challenge",
            "league",
            "tournament",
            "deck",
            "card",
            "player",
            "winner",
            "finalist",
            "result",
            "standing",
            "ranking",
            "magic",
            "mtg",
            "mtgo",
            "constructed",
            "limited",
            "draft",
            "sealed",
            "swiss",
            "elimination",
            "bracket",
            "round",
        ]

        text_lower = text.lower()
        return any(keyword in text_lower for keyword in mtg_keywords)

    def create_data_item(self, element, text: str, link_info: dict) -> dict:
        """Cr√©e un √©l√©ment de donn√©es"""
        # Chercher des dates
        found_date = self.extract_date(text)

        # Chercher le format
        detected_format = self.detect_format(text)

        # Chercher le type de contenu
        content_type = self.detect_content_type(text)

        return {
            "type": content_type,
            "title": text[:200] + "..." if len(text) > 200 else text,
            "date": found_date,
            "format": detected_format,
            "source_url": link_info["url"],
            "source_text": link_info["text"],
            "extracted_at": datetime.now().isoformat(),
        }

    def extract_date(self, text: str) -> str:
        """Extrait une date du texte"""
        date_patterns = [
            r"(\d{4}-\d{2}-\d{2})",
            r"(\d{2}/\d{2}/\d{4})",
            r"(\d{2}-\d{2}-\d{4})",
            r"(\w+ \d{1,2},? \d{4})",
            r"(\d{1,2}/\d{1,2}/\d{2,4})",
        ]

        for pattern in date_patterns:
            date_matches = re.findall(pattern, text)
            if date_matches:
                try:
                    date_str = date_matches[0]
                    if "/" in date_str:
                        if len(date_str.split("/")[2]) == 2:
                            found_date = datetime.strptime(date_str, "%m/%d/%y")
                        else:
                            found_date = datetime.strptime(date_str, "%m/%d/%Y")
                    elif "-" in date_str and len(date_str.split("-")[0]) == 2:
                        found_date = datetime.strptime(date_str, "%m-%d-%Y")
                    elif "-" in date_str and len(date_str.split("-")[0]) == 4:
                        found_date = datetime.strptime(date_str, "%Y-%m-%d")
                    else:
                        found_date = datetime.strptime(date_str, "%B %d, %Y")
                    return found_date.isoformat()
                except ValueError:
                    continue
        return None

    def detect_format(self, text: str) -> str:
        """D√©tecte le format du tournoi"""
        text_lower = text.lower()
        format_keywords = {
            "modern": "Modern",
            "standard": "Standard",
            "legacy": "Legacy",
            "pioneer": "Pioneer",
            "vintage": "Vintage",
            "pauper": "Pauper",
        }

        for keyword, format_name in format_keywords.items():
            if keyword in text_lower:
                return format_name
        return "Unknown"

    def detect_content_type(self, text: str) -> str:
        """D√©tecte le type de contenu"""
        text_lower = text.lower()

        if any(keyword in text_lower for keyword in ["deck", "list", "card"]):
            return "decklist"
        elif any(
            keyword in text_lower
            for keyword in ["tournament", "challenge", "league", "preliminary"]
        ):
            return "tournament"
        elif any(
            keyword in text_lower
            for keyword in ["result", "standing", "ranking", "winner"]
        ):
            return "result"
        elif any(
            keyword in text_lower for keyword in ["news", "announcement", "update"]
        ):
            return "news"
        else:
            return "general"

    def save_real_data(self, data: list):
        """Sauvegarde les vraies donn√©es (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # R√àGLE ABSOLUE : V√©rifier que le fichier n'existe pas d√©j√†
        json_file = self.cache_dir / f"mtgo_real_data_{timestamp}.json"

        # Si le fichier existe, ajouter un suffixe unique
        counter = 1
        while json_file.exists():
            json_file = self.cache_dir / f"mtgo_real_data_{timestamp}_{counter}.json"
            counter += 1

        # Sauvegarder en JSON (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

        # Organiser par date si disponible (AJOUT SEULEMENT)
        dated_data = [item for item in data if item.get("date")]
        if dated_data:
            self.organize_by_date(dated_data)

    def organize_by_date(self, dated_data: list):
        """Organise les donn√©es par date (AJOUT SEULEMENT)"""
        for item in dated_data:
            try:
                item_date = datetime.fromisoformat(item["date"])
                year_month_dir = (
                    self.cache_dir / str(item_date.year) / f"{item_date.month:02d}"
                )
                year_month_dir.mkdir(parents=True, exist_ok=True)

                # R√àGLE ABSOLUE : Nom de fichier unique pour √©viter l'√©crasement
                timestamp = datetime.now().strftime("%H%M%S_%f")[
                    :-3
                ]  # Microsecondes pour unicit√©
                filename = f"mtgo_real_{item_date.strftime('%Y%m%d')}_{timestamp}.json"

                # V√©rifier que le fichier n'existe pas d√©j√†
                file_path = year_month_dir / filename
                counter = 1
                while file_path.exists():
                    filename = f"mtgo_real_{item_date.strftime('%Y%m%d')}_{timestamp}_{counter}.json"
                    file_path = year_month_dir / filename
                    counter += 1

                # Sauvegarder (AJOUT SEULEMENT)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            **item,
                            "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                            "git_rule": "CACHE_NOT_COMMITTED",
                        },
                        f,
                        indent=2,
                    )

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur organisation par date: {e}")


def main():
    """Fonction principale"""
    scraper = MTGORealDataScraper()

    print("üéØ SCRAPER MTGO DONN√âES R√âELLES")
    print("=" * 50)
    print("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
    print("üìã Seulement AJOUTER de nouvelles donn√©es")
    print("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
    print("=" * 50)
    print("üîç Strat√©gie :")
    print("   1. Explorer toutes les pages MTGO")
    print("   2. Identifier les vraies URLs de donn√©es")
    print("   3. Scraper les donn√©es r√©elles")
    print("=" * 50)

    # √âtape 1: Trouver les vraies URLs de donn√©es
    data_links = scraper.find_real_data_urls()

    # √âtape 2: Scraper les vraies donn√©es
    if data_links:
        scraper.scrape_real_data(data_links)
    else:
        print("‚ùå Aucune URL de donn√©es trouv√©e")

    print("\n‚úÖ Scraping des donn√©es r√©elles termin√©!")
    print("‚úÖ R√àGLE RESPECT√âE : Cache existant pr√©serv√©")


if __name__ == "__main__":
    main()
