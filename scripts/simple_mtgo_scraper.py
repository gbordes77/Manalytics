#!/usr/bin/env python3
"""
Scraper MTGO Simple et Efficace - R√©cup√®re les vraies donn√©es de tournois

R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT
- Tous les fichiers existants doivent √™tre pr√©serv√©s
- Seulement AJOUTER de nouvelles donn√©es
- Aucune suppression, remplacement ou √©crasement autoris√©
"""

import json
import logging
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

import requests
from bs4 import BeautifulSoup

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class SimpleMTGOScraper:
    """Scraper MTGO simple qui r√©cup√®re les vraies donn√©es"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        self.cache_dir = Path("data/raw/mtgo")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

        # R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE
        self.logger.info("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
        self.logger.info("üìã Seulement AJOUTER de nouvelles donn√©es")

    def scrape_mtgo_data(self):
        """Scrape les donn√©es MTGO de mani√®re simple et efficace"""
        self.logger.info("üöÄ D√©but du scraping MTGO simple")

        # V√©rifier le cache existant AVANT de commencer
        existing_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(f"üìÅ Cache existant : {len(existing_files)} fichiers pr√©serv√©s")

        # URLs directes des tournois MTGO
        tournament_urls = [
            "https://www.mtgo.com/en/mtgo/tournaments/modern-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/standard-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/legacy-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/pioneer-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/vintage-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/pauper-challenge",
            "https://www.mtgo.com/en/mtgo/tournaments/modern-league",
            "https://www.mtgo.com/en/mtgo/tournaments/standard-league",
            "https://www.mtgo.com/en/mtgo/tournaments/legacy-league",
            "https://www.mtgo.com/en/mtgo/tournaments/pioneer-league",
        ]

        all_data = []

        for url in tournament_urls:
            self.logger.info(f"üì° Scraping: {url}")

            try:
                response = self.session.get(url, timeout=30)
                if response.status_code == 200:
                    data = self.extract_tournament_data(response.text, url)
                    all_data.extend(data)
                    self.logger.info(f"‚úÖ {len(data)} tournois trouv√©s sur {url}")
                else:
                    self.logger.warning(f"‚ùå Erreur {response.status_code} pour {url}")

                # Pause pour √©viter d'√™tre bloqu√©
                time.sleep(2)

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur avec {url}: {e}")

        # Sauvegarder toutes les donn√©es (AJOUT SEULEMENT)
        if all_data:
            self.save_all_data(all_data)
        else:
            self.logger.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e")

        # V√©rifier le cache APR√àS sauvegarde
        final_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(
            f"üìÅ Cache final : {len(final_files)} fichiers (pr√©server + ajout)"
        )
        self.logger.info(
            f"‚úÖ R√àGLE RESPECT√âE : {len(final_files) - len(existing_files)} nouveaux fichiers ajout√©s"
        )

    def extract_tournament_data(self, html_content: str, source_url: str) -> list:
        """Extrait les donn√©es de tournois du HTML"""
        data = []
        soup = BeautifulSoup(html_content, "html.parser")

        # Chercher les √©l√©ments de tournois
        tournament_elements = soup.find_all(
            ["div", "article", "section"],
            class_=re.compile(r"tournament|challenge|league|result", re.IGNORECASE),
        )

        if not tournament_elements:
            # Chercher dans tout le contenu
            tournament_elements = soup.find_all(["div", "article", "section"])

        for element in tournament_elements:
            text = element.get_text(strip=True)

            # Chercher des informations de tournois
            if any(
                keyword in text.lower()
                for keyword in [
                    "modern",
                    "standard",
                    "legacy",
                    "pioneer",
                    "vintage",
                    "pauper",
                    "challenge",
                    "league",
                ]
            ):

                # Chercher des dates
                date_patterns = [
                    r"(\d{4}-\d{2}-\d{2})",
                    r"(\d{2}/\d{2}/\d{4})",
                    r"(\d{2}-\d{2}-\d{4})",
                    r"(\w+ \d{1,2},? \d{4})",
                ]

                found_date = None
                for pattern in date_patterns:
                    date_matches = re.findall(pattern, text)
                    if date_matches:
                        try:
                            date_str = date_matches[0]
                            if "/" in date_str:
                                found_date = datetime.strptime(date_str, "%m/%d/%Y")
                            elif "-" in date_str and len(date_str.split("-")[0]) == 2:
                                found_date = datetime.strptime(date_str, "%m-%d-%Y")
                            elif "-" in date_str and len(date_str.split("-")[0]) == 4:
                                found_date = datetime.strptime(date_str, "%Y-%m-%d")
                            else:
                                # Format "Month Day, Year"
                                found_date = datetime.strptime(date_str, "%B %d, %Y")
                            break
                        except ValueError:
                            continue

                # Cr√©er l'√©l√©ment de donn√©es
                data_item = {
                    "title": text[:200] + "..." if len(text) > 200 else text,
                    "date": found_date.isoformat() if found_date else None,
                    "source_url": source_url,
                    "extracted_at": datetime.now().isoformat(),
                    "format": self.detect_format(text),
                }
                data.append(data_item)

        return data

    def detect_format(self, text: str) -> str:
        """D√©tecte le format du tournoi"""
        text_lower = text.lower()
        if "modern" in text_lower:
            return "Modern"
        elif "standard" in text_lower:
            return "Standard"
        elif "legacy" in text_lower:
            return "Legacy"
        elif "pioneer" in text_lower:
            return "Pioneer"
        elif "vintage" in text_lower:
            return "Vintage"
        elif "pauper" in text_lower:
            return "Pauper"
        else:
            return "Unknown"

    def save_all_data(self, data: list):
        """Sauvegarde toutes les donn√©es trouv√©es (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # R√àGLE ABSOLUE : V√©rifier que le fichier n'existe pas d√©j√†
        json_file = self.cache_dir / f"mtgo_simple_data_{timestamp}.json"

        # Si le fichier existe, ajouter un suffixe unique
        counter = 1
        while json_file.exists():
            json_file = self.cache_dir / f"mtgo_simple_data_{timestamp}_{counter}.json"
            counter += 1

        # Sauvegarder en JSON (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

        # Organiser par date si disponible (AJOUT SEULEMENT)
        dated_data = [item for item in data if item.get("date")]
        if dated_data:
            self.organize_by_date(dated_data)

    def organize_by_date(self, dated_data: list):
        """Organise les donn√©es par date (AJOUT SEULEMENT)"""
        for item in dated_data:
            try:
                item_date = datetime.fromisoformat(item["date"])
                year_month_dir = (
                    self.cache_dir / str(item_date.year) / f"{item_date.month:02d}"
                )
                year_month_dir.mkdir(parents=True, exist_ok=True)

                # R√àGLE ABSOLUE : Nom de fichier unique pour √©viter l'√©crasement
                timestamp = datetime.now().strftime("%H%M%S_%f")[
                    :-3
                ]  # Microsecondes pour unicit√©
                filename = (
                    f"mtgo_simple_{item_date.strftime('%Y%m%d')}_{timestamp}.json"
                )

                # V√©rifier que le fichier n'existe pas d√©j√†
                file_path = year_month_dir / filename
                counter = 1
                while file_path.exists():
                    filename = f"mtgo_simple_{item_date.strftime('%Y%m%d')}_{timestamp}_{counter}.json"
                    file_path = year_month_dir / filename
                    counter += 1

                # Sauvegarder (AJOUT SEULEMENT)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            **item,
                            "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                            "git_rule": "CACHE_NOT_COMMITTED",
                        },
                        f,
                        indent=2,
                    )

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur organisation par date: {e}")


def main():
    """Fonction principale"""
    scraper = SimpleMTGOScraper()

    print("üéØ SCRAPER MTGO SIMPLE ET EFFICACE")
    print("=" * 50)
    print("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
    print("üìã Seulement AJOUTER de nouvelles donn√©es")
    print("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
    print("=" * 50)

    scraper.scrape_mtgo_data()

    print("\n‚úÖ Scraping termin√©!")
    print("‚úÖ R√àGLE RESPECT√âE : Cache existant pr√©serv√©")


if __name__ == "__main__":
    main()
