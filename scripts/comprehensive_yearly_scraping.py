#!/usr/bin/env python3
"""
Scraper complet - R√©cup√©ration de toutes les donn√©es d'aujourd'hui √† 1 an en arri√®re

R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT
- Tous les fichiers existants doivent √™tre pr√©serv√©s
- Seulement AJOUTER de nouvelles donn√©es
- Aucune suppression, remplacement ou √©crasement autoris√©
- LE CACHE NE DOIT PAS √äTRE COMMIT√â
"""

import asyncio
import json
import logging
import os
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import aiohttp
from bs4 import BeautifulSoup

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/comprehensive_scraping.log"),
        logging.StreamHandler(),
    ],
)


class ComprehensiveYearlyScraper:
    """Scraper complet pour r√©cup√©rer 1 an de donn√©es"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE
        self.logger.info("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
        self.logger.info("üìã Seulement AJOUTER de nouvelles donn√©es")
        self.logger.info("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
        self.logger.info("üö´ LE CACHE NE DOIT PAS √äTRE COMMIT√â")

        # Configuration
        self.base_urls = {
            "mtgo": "https://www.mtgo.com",
            "melee": "https://melee.gg",
            "topdeck": "https://topdeck.gg",
        }

        # Dossiers de cache (NON COMMIT√âS)
        self.cache_dirs = {
            "mtgo": Path("data/raw/mtgo"),
            "melee": Path("data/raw/melee"),
            "topdeck": Path("data/raw/topdeck"),
        }

        # Cr√©er les dossiers
        for cache_dir in self.cache_dirs.values():
            cache_dir.mkdir(parents=True, exist_ok=True)

        # V√©rifier le cache existant
        self._check_existing_cache()

    def _check_existing_cache(self):
        """V√©rifier le cache existant √† pr√©server"""
        total_files = 0
        for source, cache_dir in self.cache_dirs.items():
            if cache_dir.exists():
                files = list(cache_dir.rglob("*.json"))
                total_files += len(files)
                self.logger.info(
                    f"üìÅ {source.upper()} cache existant : {len(files)} fichiers pr√©serv√©s"
                )

        self.logger.info(f"üìä TOTAL CACHE EXISTANT : {total_files} fichiers √† pr√©server")

    async def scrape_complete_year(self):
        """Scraper complet pour 1 an de donn√©es"""
        self.logger.info("üöÄ D√âBUT SCRAPING COMPLET - 1 AN DE DONN√âES")

        # Calculer la p√©riode (aujourd'hui √† 1 an en arri√®re)
        today = datetime.now()
        one_year_ago = today - timedelta(days=365)

        self.logger.info(
            f"üìÖ P√©riode : {one_year_ago.strftime('%Y-%m-%d')} √† {today.strftime('%Y-%m-%d')}"
        )

        # V√©rifier le cache avant scraping
        cache_before = self._count_total_cache_files()
        self.logger.info(
            f"üìÅ Cache avant scraping : {cache_before} fichiers √† pr√©server"
        )

        # Scraper chaque source
        results = {}

        # 1. Scraping MTGO
        self.logger.info("üåê SCRAPING MTGO COMPLET...")
        results["mtgo"] = await self._scrape_mtgo_complete(one_year_ago, today)

        # 2. Scraping Melee
        self.logger.info("üåê SCRAPING MELEE COMPLET...")
        results["melee"] = await self._scrape_melee_complete(one_year_ago, today)

        # 3. Scraping TopDeck
        self.logger.info("üåê SCRAPING TOPDECK COMPLET...")
        results["topdeck"] = await self._scrape_topdeck_complete(one_year_ago, today)

        # V√©rifier que le cache a √©t√© pr√©serv√©
        cache_after = self._count_total_cache_files()
        new_files = cache_after - cache_before

        self.logger.info(
            f"‚úÖ R√àGLE CACHE RESPECT√âE : {new_files} nouveaux fichiers ajout√©s"
        )
        self.logger.info(
            f"üìÅ Cache final : {cache_after} fichiers ({cache_before} pr√©serv√©s + {new_files} ajout√©s)"
        )

        # R√©sum√© final
        self._generate_final_report(results, cache_before, cache_after)

        return results

    def _count_total_cache_files(self):
        """Compter tous les fichiers dans le cache"""
        total = 0
        for cache_dir in self.cache_dirs.values():
            if cache_dir.exists():
                total += len(list(cache_dir.rglob("*.json")))
        return total

    async def _scrape_mtgo_complete(self, start_date: datetime, end_date: datetime):
        """Scraping complet MTGO"""
        self.logger.info(
            f"üì° Scraping MTGO : {start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}"
        )

        mtgo_data = []

        # URLs MTGO √† scraper
        mtgo_urls = [
            "https://www.mtgo.com/en/mtgo/tournaments",
            "https://www.mtgo.com/en/mtgo/results",
            "https://www.mtgo.com/en/mtgo/standings",
            "https://www.mtgo.com/decklists",
            "https://www.mtgo.com/news",
        ]

        async with aiohttp.ClientSession() as session:
            for url in mtgo_urls:
                try:
                    self.logger.info(f"üîç Scraping MTGO URL: {url}")

                    async with session.get(url, timeout=30) as response:
                        if response.status == 200:
                            content = await response.text()

                            # Extraire les donn√©es de cette page
                            page_data = await self._extract_mtgo_data(
                                content, url, start_date, end_date
                            )
                            mtgo_data.extend(page_data)

                            self.logger.info(
                                f"‚úÖ {len(page_data)} √©l√©ments trouv√©s sur {url}"
                            )

                            # Suivre les liens pour plus de donn√©es
                            links = await self._extract_mtgo_links(content, url)
                            for link in links[:5]:  # Limiter √† 5 liens par page
                                link_data = await self._scrape_mtgo_link(
                                    session, link, start_date, end_date
                                )
                                mtgo_data.extend(link_data)

                except Exception as e:
                    self.logger.error(f"‚ùå Erreur avec {url}: {e}")

        # Sauvegarder les donn√©es MTGO (AJOUT SEULEMENT)
        if mtgo_data:
            await self._save_mtgo_data(mtgo_data)

        return {
            "source": "mtgo",
            "total_items": len(mtgo_data),
            "period": f"{start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}",
        }

    async def _scrape_melee_complete(self, start_date: datetime, end_date: datetime):
        """Scraping complet Melee"""
        self.logger.info(
            f"üì° Scraping Melee : {start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}"
        )

        melee_data = []

        # URLs Melee √† scraper
        melee_urls = [
            "https://melee.gg/Tournament/",
            "https://melee.gg/Events/",
            "https://melee.gg/Results/",
        ]

        async with aiohttp.ClientSession() as session:
            for url in melee_urls:
                try:
                    self.logger.info(f"üîç Scraping Melee URL: {url}")

                    async with session.get(url, timeout=30) as response:
                        if response.status == 200:
                            content = await response.text()

                            # Extraire les donn√©es de cette page
                            page_data = await self._extract_melee_data(
                                content, url, start_date, end_date
                            )
                            melee_data.extend(page_data)

                            self.logger.info(
                                f"‚úÖ {len(page_data)} √©l√©ments trouv√©s sur {url}"
                            )

                except Exception as e:
                    self.logger.error(f"‚ùå Erreur avec {url}: {e}")

        # Sauvegarder les donn√©es Melee (AJOUT SEULEMENT)
        if melee_data:
            await self._save_melee_data(melee_data)

        return {
            "source": "melee",
            "total_items": len(melee_data),
            "period": f"{start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}",
        }

    async def _scrape_topdeck_complete(self, start_date: datetime, end_date: datetime):
        """Scraping complet TopDeck"""
        self.logger.info(
            f"üì° Scraping TopDeck : {start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}"
        )

        topdeck_data = []

        # URLs TopDeck √† scraper
        topdeck_urls = [
            "https://topdeck.gg/",
            "https://topdeck.gg/tournaments",
            "https://topdeck.gg/results",
        ]

        async with aiohttp.ClientSession() as session:
            for url in topdeck_urls:
                try:
                    self.logger.info(f"üîç Scraping TopDeck URL: {url}")

                    async with session.get(url, timeout=30) as response:
                        if response.status == 200:
                            content = await response.text()

                            # Extraire les donn√©es de cette page
                            page_data = await self._extract_topdeck_data(
                                content, url, start_date, end_date
                            )
                            topdeck_data.extend(page_data)

                            self.logger.info(
                                f"‚úÖ {len(page_data)} √©l√©ments trouv√©s sur {url}"
                            )

                except Exception as e:
                    self.logger.error(f"‚ùå Erreur avec {url}: {e}")

        # Sauvegarder les donn√©es TopDeck (AJOUT SEULEMENT)
        if topdeck_data:
            await self._save_topdeck_data(topdeck_data)

        return {
            "source": "topdeck",
            "total_items": len(topdeck_data),
            "period": f"{start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}",
        }

    async def _extract_mtgo_data(
        self, content: str, source_url: str, start_date: datetime, end_date: datetime
    ) -> List[Dict]:
        """Extrait les donn√©es MTGO d'une page"""
        data = []
        soup = BeautifulSoup(content, "html.parser")

        # Chercher des informations de tournois
        tournament_elements = soup.find_all(
            string=re.compile(
                r"Modern|Standard|Legacy|Pioneer|Vintage|Pauper|Challenge|League|Preliminary",
                re.IGNORECASE,
            )
        )

        for element in tournament_elements:
            if element.strip():
                # Chercher des dates dans le contexte
                parent = element.parent
                if parent:
                    # Chercher des dates
                    date_patterns = [
                        r"(\d{4}-\d{2}-\d{2})",
                        r"(\d{2}/\d{2}/\d{4})",
                        r"(\d{2}-\d{2}-\d{4})",
                    ]

                    for pattern in date_patterns:
                        date_matches = re.findall(pattern, str(parent))
                        for date_match in date_matches:
                            try:
                                if "/" in date_match:
                                    parsed_date = datetime.strptime(
                                        date_match, "%m/%d/%Y"
                                    )
                                elif (
                                    "-" in date_match
                                    and len(date_match.split("-")[0]) == 2
                                ):
                                    parsed_date = datetime.strptime(
                                        date_match, "%m-%d-%Y"
                                    )
                                else:
                                    parsed_date = datetime.strptime(
                                        date_match, "%Y-%m-%d"
                                    )

                                # V√©rifier que la date est dans la p√©riode
                                if start_date <= parsed_date <= end_date:
                                    data_item = {
                                        "title": element.strip(),
                                        "date": parsed_date.isoformat(),
                                        "source_url": source_url,
                                        "source": "mtgo",
                                        "extracted_at": datetime.now().isoformat(),
                                    }
                                    data.append(data_item)

                            except ValueError:
                                continue

        return data

    async def _extract_melee_data(
        self, content: str, source_url: str, start_date: datetime, end_date: datetime
    ) -> List[Dict]:
        """Extrait les donn√©es Melee d'une page"""
        data = []
        soup = BeautifulSoup(content, "html.parser")

        # Chercher des informations de tournois
        tournament_elements = soup.find_all(
            string=re.compile(
                r"Modern|Standard|Legacy|Pioneer|Vintage|Pauper|Tournament|Event",
                re.IGNORECASE,
            )
        )

        for element in tournament_elements:
            if element.strip():
                # Logique similaire √† MTGO
                data_item = {
                    "title": element.strip(),
                    "date": None,  # √Ä extraire si possible
                    "source_url": source_url,
                    "source": "melee",
                    "extracted_at": datetime.now().isoformat(),
                }
                data.append(data_item)

        return data

    async def _extract_topdeck_data(
        self, content: str, source_url: str, start_date: datetime, end_date: datetime
    ) -> List[Dict]:
        """Extrait les donn√©es TopDeck d'une page"""
        data = []
        soup = BeautifulSoup(content, "html.parser")

        # Chercher des informations de tournois
        tournament_elements = soup.find_all(
            string=re.compile(
                r"Modern|Standard|Legacy|Pioneer|Vintage|Pauper|Tournament|Event",
                re.IGNORECASE,
            )
        )

        for element in tournament_elements:
            if element.strip():
                # Logique similaire √† MTGO
                data_item = {
                    "title": element.strip(),
                    "date": None,  # √Ä extraire si possible
                    "source_url": source_url,
                    "source": "topdeck",
                    "extracted_at": datetime.now().isoformat(),
                }
                data.append(data_item)

        return data

    async def _extract_mtgo_links(self, content: str, source_url: str) -> List[Dict]:
        """Extrait les liens MTGO d'une page"""
        links = []
        soup = BeautifulSoup(content, "html.parser")

        all_links = soup.find_all("a", href=True)

        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True)

            # Filtrer les liens pertinents
            if any(
                keyword in href.lower() or keyword in text.lower()
                for keyword in [
                    "decklist",
                    "tournament",
                    "result",
                    "challenge",
                    "league",
                    "news",
                ]
            ):

                # Construire l'URL compl√®te
                if href.startswith("/"):
                    full_url = f"{self.base_urls['mtgo']}{href}"
                elif href.startswith("http"):
                    full_url = href
                else:
                    full_url = f"{self.base_urls['mtgo']}/{href}"

                links.append({"url": full_url, "text": text, "source": source_url})

        return links

    async def _scrape_mtgo_link(
        self,
        session: aiohttp.ClientSession,
        link_info: Dict,
        start_date: datetime,
        end_date: datetime,
    ) -> List[Dict]:
        """Scrape les donn√©es d'un lien MTGO sp√©cifique"""
        link_data = []

        try:
            self.logger.info(
                f"üîç Suivi du lien MTGO: {link_info['text']} -> {link_info['url']}"
            )

            async with session.get(link_info["url"], timeout=15) as response:
                if response.status == 200:
                    content = await response.text()

                    # Analyser le contenu de cette page
                    page_data = await self._extract_mtgo_data(
                        content, link_info["url"], start_date, end_date
                    )
                    link_data.extend(page_data)

                    self.logger.info(
                        f"‚úÖ {len(page_data)} √©l√©ments trouv√©s sur {link_info['url']}"
                    )
                else:
                    self.logger.warning(
                        f"‚ùå Erreur {response.status} pour {link_info['url']}"
                    )

        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Erreur avec le lien {link_info['url']}: {e}")

        return link_data

    async def _save_mtgo_data(self, data: List[Dict]):
        """Sauvegarde les donn√©es MTGO (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

        # R√àGLE ABSOLUE : Nom de fichier unique
        json_file = self.cache_dirs["mtgo"] / f"mtgo_complete_{timestamp}.json"

        # V√©rifier que le fichier n'existe pas d√©j√†
        counter = 1
        while json_file.exists():
            json_file = (
                self.cache_dirs["mtgo"] / f"mtgo_complete_{timestamp}_{counter}.json"
            )
            counter += 1

        # Sauvegarder (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments MTGO sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

    async def _save_melee_data(self, data: List[Dict]):
        """Sauvegarde les donn√©es Melee (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

        # R√àGLE ABSOLUE : Nom de fichier unique
        json_file = self.cache_dirs["melee"] / f"melee_complete_{timestamp}.json"

        # V√©rifier que le fichier n'existe pas d√©j√†
        counter = 1
        while json_file.exists():
            json_file = (
                self.cache_dirs["melee"] / f"melee_complete_{timestamp}_{counter}.json"
            )
            counter += 1

        # Sauvegarder (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments Melee sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

    async def _save_topdeck_data(self, data: List[Dict]):
        """Sauvegarde les donn√©es TopDeck (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

        # R√àGLE ABSOLUE : Nom de fichier unique
        json_file = self.cache_dirs["topdeck"] / f"topdeck_complete_{timestamp}.json"

        # V√©rifier que le fichier n'existe pas d√©j√†
        counter = 1
        while json_file.exists():
            json_file = (
                self.cache_dirs["topdeck"]
                / f"topdeck_complete_{timestamp}_{counter}.json"
            )
            counter += 1

        # Sauvegarder (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments TopDeck sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

    def _generate_final_report(
        self, results: Dict, cache_before: int, cache_after: int
    ):
        """G√©n√®re un rapport final du scraping"""
        self.logger.info("üìä RAPPORT FINAL DU SCRAPING COMPLET")
        self.logger.info("=" * 50)

        total_items = 0
        for source, result in results.items():
            items = result["total_items"]
            total_items += items
            self.logger.info(f"üåê {source.upper()}: {items} √©l√©ments")

        new_files = cache_after - cache_before
        self.logger.info(f"üìÅ Nouveaux fichiers ajout√©s: {new_files}")
        self.logger.info(f"üìÅ Fichiers pr√©serv√©s: {cache_before}")
        self.logger.info(f"üìÅ Total final: {cache_after}")
        self.logger.info(f"üìä Total √©l√©ments r√©cup√©r√©s: {total_items}")

        self.logger.info("‚úÖ R√àGLE ABSOLUE RESPECT√âE: Cache pr√©serv√©, seulement ajouts")
        self.logger.info("üö´ CACHE NON COMMIT√â: Donn√©es locales uniquement")


async def main():
    """Fonction principale"""
    scraper = ComprehensiveYearlyScraper()

    print("üéØ SCRAPING COMPLET - 1 AN DE DONN√âES")
    print("=" * 50)
    print("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
    print("üìã Seulement AJOUTER de nouvelles donn√©es")
    print("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
    print("üö´ LE CACHE NE DOIT PAS √äTRE COMMIT√â")
    print("=" * 50)

    results = await scraper.scrape_complete_year()

    print("\n‚úÖ Scraping complet termin√©!")
    print("‚úÖ R√àGLE RESPECT√âE : Cache existant pr√©serv√©")
    print("‚úÖ CACHE NON COMMIT√â : Donn√©es locales uniquement")


if __name__ == "__main__":
    asyncio.run(main())
