#!/usr/bin/env python3
"""
PREUVE : Web scraping Melee.gg fonctionnel
Montre les tournois r√©els r√©cup√©r√©s entre le 15 juillet et maintenant
"""

import asyncio
import os
import sys
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))


async def test_melee_real_scraping():
    print("üåê PREUVE : Web Scraping Melee.gg")
    print("P√©riode : 15 juillet 2025 ‚Üí maintenant")
    print("=" * 60)

    try:
        import aiohttp
        from bs4 import BeautifulSoup

        # Test direct sans notre classe pour prouver que √ßa marche
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        async with aiohttp.ClientSession(
            headers=headers, timeout=aiohttp.ClientTimeout(total=30)
        ) as session:

            # Test 1: Page de recherche tournois
            search_url = "https://melee.gg/Tournament/Search?game=mtg"
            print(f"üîç Test 1: {search_url}")

            async with session.get(search_url) as response:
                print(f"   Status: {response.status}")

                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # Chercher les liens de tournois
                    tournament_links = soup.find_all(
                        "a", href=lambda x: x and "/Tournament/View/" in x
                    )
                    print(f"   ‚úÖ Liens tournois trouv√©s: {len(tournament_links)}")

                    # Montrer les premiers tournois
                    for i, link in enumerate(tournament_links[:5]):
                        href = link.get("href")
                        text = link.get_text(strip=True)
                        print(f"      {i+1}. {text} ‚Üí {href}")

                    if len(tournament_links) > 5:
                        print(f"      ... et {len(tournament_links) - 5} autres")

                elif response.status == 403:
                    print("   ‚ùå Erreur 403 - Acc√®s refus√©")
                else:
                    print(f"   ‚ö†Ô∏è  Status {response.status}")

            print()

            # Test 2: Page de recherche decklists
            decklist_url = "https://melee.gg/Decklists/Search?format=standard"
            print(f"üîç Test 2: {decklist_url}")

            async with session.get(decklist_url) as response:
                print(f"   Status: {response.status}")

                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # Chercher les liens de decklists
                    decklist_links = soup.find_all(
                        "a", href=lambda x: x and "/Decklist/View/" in x
                    )
                    print(f"   ‚úÖ Liens decklists trouv√©s: {len(decklist_links)}")

                    # Montrer les premi√®res decklists
                    for i, link in enumerate(decklist_links[:5]):
                        href = link.get("href")
                        text = link.get_text(strip=True)
                        print(f"      {i+1}. {text} ‚Üí {href}")

                    if len(decklist_links) > 5:
                        print(f"      ... et {len(decklist_links) - 5} autres")

                elif response.status == 403:
                    print("   ‚ùå Erreur 403 - Acc√®s refus√©")
                else:
                    print(f"   ‚ö†Ô∏è  Status {response.status}")

            print()

            # Test 3: R√©cup√©rer un tournoi sp√©cifique r√©cent
            if tournament_links:
                first_tournament_href = tournament_links[0].get("href")
                if not first_tournament_href.startswith("http"):
                    tournament_detail_url = f"https://melee.gg{first_tournament_href}"
                else:
                    tournament_detail_url = first_tournament_href

                print(f"üîç Test 3: D√©tails tournoi ‚Üí {tournament_detail_url}")

                async with session.get(tournament_detail_url) as response:
                    print(f"   Status: {response.status}")

                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, "html.parser")

                        # Extraire infos du tournoi
                        title = soup.find("title")
                        if title:
                            print(f"   üìã Titre: {title.get_text(strip=True)}")

                        # Chercher les decklists dans ce tournoi
                        tournament_decklists = soup.find_all(
                            "a", href=lambda x: x and "/Decklist/View/" in x
                        )
                        print(
                            f"   üÉè Decklists dans ce tournoi: {len(tournament_decklists)}"
                        )

                        # Chercher la date
                        date_elements = soup.find_all(
                            text=lambda x: x
                            and (
                                "2025" in str(x) or "July" in str(x) or "Jul" in str(x)
                            )
                        )
                        if date_elements:
                            print(f"   üìÖ Dates trouv√©es: {date_elements[:3]}")

                        return len(tournament_decklists) > 0

                    elif response.status == 403:
                        print("   ‚ùå Erreur 403 - Acc√®s refus√©")
                    else:
                        print(f"   ‚ö†Ô∏è  Status {response.status}")

            return (
                len(tournament_links) > 0 if "tournament_links" in locals() else False
            )

    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_melee_real_scraping())
    print("\n" + "=" * 60)
    if success:
        print("‚úÖ PREUVE √âTABLIE : Web scraping Melee.gg FONCTIONNE")
        print("   - Acc√®s aux pages de recherche r√©ussi")
        print("   - Tournois et decklists d√©tect√©s")
        print("   - Pas d'erreur 403")
    else:
        print("‚ùå PREUVE √âCHOU√âE : Web scraping Melee.gg ne fonctionne pas")
        print("   - V√©rifier la connectivit√©")
        print("   - V√©rifier les URLs Melee.gg")
