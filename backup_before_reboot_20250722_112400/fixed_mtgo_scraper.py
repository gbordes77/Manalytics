#!/usr/bin/env python3
"""
Scraper MTGO corrig√© - Suit les liens et scrape les vraies donn√©es

R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT
- Tous les fichiers existants doivent √™tre pr√©serv√©s
- Seulement AJOUTER de nouvelles donn√©es
- Aucune suppression, remplacement ou √©crasement autoris√©
"""

import asyncio
import json
import logging
import re
from datetime import datetime, timedelta
from pathlib import Path

import aiohttp
from bs4 import BeautifulSoup

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class FixedMTGOScraper:
    """Scraper MTGO qui suit les liens et trouve les vraies donn√©es"""

    def __init__(self):
        self.base_url = "https://www.mtgo.com"
        self.cache_dir = Path("data/raw/mtgo")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

        # R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE
        self.logger.info("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
        self.logger.info("üìã Seulement AJOUTER de nouvelles donn√©es")
        self.logger.info("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
        self.logger.info("üö´ LE CACHE NE DOIT PAS √äTRE COMMIT√â")

    async def scrape_mtgo_data(self):
        """Scrape les donn√©es MTGO en suivant les liens"""
        self.logger.info("üöÄ D√©but du scraping MTGO corrig√©")

        # V√©rifier le cache existant AVANT de commencer
        existing_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(f"üìÅ Cache existant : {len(existing_files)} fichiers pr√©serv√©s")

        # URLs principales
        main_urls = [
            "https://www.mtgo.com/en/mtgo/tournaments",
            "https://www.mtgo.com/en/mtgo/results",
            "https://www.mtgo.com/en/mtgo/standings",
        ]

        all_data = []

        async with aiohttp.ClientSession() as session:
            for main_url in main_urls:
                self.logger.info(f"üì° Scraping page principale: {main_url}")

                try:
                    async with session.get(main_url, timeout=15) as response:
                        if response.status == 200:
                            content = await response.text()

                            # 1. Extraire les liens de cette page
                            links = await self.extract_links(content, main_url)
                            self.logger.info(
                                f"üîó Trouv√© {len(links)} liens sur {main_url}"
                            )

                            # 2. Suivre chaque lien et scraper les donn√©es
                            for link in links:
                                link_data = await self.scrape_link_data(session, link)
                                if link_data:
                                    all_data.extend(link_data)

                except Exception as e:
                    self.logger.error(f"‚ö†Ô∏è Erreur avec {main_url}: {e}")

        # Sauvegarder toutes les donn√©es (AJOUT SEULEMENT)
        if all_data:
            await self.save_all_data(all_data)
        else:
            self.logger.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e")

        # V√©rifier le cache APR√àS sauvegarde
        final_files = list(self.cache_dir.rglob("*.json"))
        self.logger.info(
            f"üìÅ Cache final : {len(final_files)} fichiers (pr√©server + ajout)"
        )
        self.logger.info(
            f"‚úÖ R√àGLE RESPECT√âE : {len(final_files) - len(existing_files)} nouveaux fichiers ajout√©s"
        )

    async def extract_links(self, content: str, source_url: str) -> list:
        """Extrait tous les liens pertinents d'une page"""
        links = []
        soup = BeautifulSoup(content, "html.parser")

        # Chercher tous les liens
        all_links = soup.find_all("a", href=True)

        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True)

            # Filtrer les liens pertinents
            if any(
                keyword in href.lower() or keyword in text.lower()
                for keyword in [
                    "decklist",
                    "tournament",
                    "result",
                    "challenge",
                    "league",
                ]
            ):

                # Construire l'URL compl√®te
                if href.startswith("/"):
                    full_url = f"{self.base_url}{href}"
                elif href.startswith("http"):
                    full_url = href
                else:
                    full_url = f"{self.base_url}/{href}"

                links.append({"url": full_url, "text": text, "source": source_url})

        return links

    async def scrape_link_data(
        self, session: aiohttp.ClientSession, link_info: dict
    ) -> list:
        """Scrape les donn√©es d'un lien sp√©cifique"""
        link_data = []

        try:
            self.logger.info(
                f"üîç Suivi du lien: {link_info['text']} -> {link_info['url']}"
            )

            async with session.get(link_info["url"], timeout=15) as response:
                if response.status == 200:
                    content = await response.text()

                    # Analyser le contenu de cette page
                    page_data = await self.analyze_page_content(content, link_info)
                    link_data.extend(page_data)

                    self.logger.info(
                        f"‚úÖ {len(page_data)} √©l√©ments trouv√©s sur {link_info['url']}"
                    )
                else:
                    self.logger.warning(
                        f"‚ùå Erreur {response.status} pour {link_info['url']}"
                    )

        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Erreur avec le lien {link_info['url']}: {e}")

        return link_data

    async def analyze_page_content(self, content: str, link_info: dict) -> list:
        """Analyse le contenu d'une page pour extraire les donn√©es"""
        page_data = []
        soup = BeautifulSoup(content, "html.parser")

        # Chercher des informations de tournois
        tournament_info = soup.find_all(
            string=re.compile(
                r"Modern|Standard|Legacy|Pioneer|Vintage|Pauper|Challenge|League",
                re.IGNORECASE,
            )
        )

        for info in tournament_info:
            if info.strip():
                # Chercher des dates dans le contexte
                parent = info.parent
                if parent:
                    # Chercher des dates
                    date_patterns = [
                        r"(\d{4}-\d{2}-\d{2})",
                        r"(\d{2}/\d{2}/\d{4})",
                        r"(\d{2}-\d{2}-\d{4})",
                    ]

                    for pattern in date_patterns:
                        date_matches = re.findall(pattern, str(parent))
                        for date_match in date_matches:
                            try:
                                if "/" in date_match:
                                    parsed_date = datetime.strptime(
                                        date_match, "%m/%d/%Y"
                                    )
                                elif (
                                    "-" in date_match
                                    and len(date_match.split("-")[0]) == 2
                                ):
                                    parsed_date = datetime.strptime(
                                        date_match, "%m-%d-%Y"
                                    )
                                else:
                                    parsed_date = datetime.strptime(
                                        date_match, "%Y-%m-%d"
                                    )

                                data_item = {
                                    "title": info.strip(),
                                    "date": parsed_date.isoformat(),
                                    "source_url": link_info["url"],
                                    "source_text": link_info["text"],
                                    "extracted_at": datetime.now().isoformat(),
                                }
                                page_data.append(data_item)

                            except ValueError:
                                continue

        # Si pas de dates trouv√©es, sauvegarder quand m√™me les informations
        if not page_data and tournament_info:
            for info in tournament_info[:5]:  # Limiter √† 5 √©l√©ments
                if info.strip():
                    data_item = {
                        "title": info.strip(),
                        "date": None,
                        "source_url": link_info["url"],
                        "source_text": link_info["text"],
                        "extracted_at": datetime.now().isoformat(),
                    }
                    page_data.append(data_item)

        return page_data

    async def save_all_data(self, data: list):
        """Sauvegarde toutes les donn√©es trouv√©es (AJOUT SEULEMENT)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # R√àGLE ABSOLUE : V√©rifier que le fichier n'existe pas d√©j√†
        json_file = self.cache_dir / f"mtgo_complete_data_{timestamp}.json"

        # Si le fichier existe, ajouter un suffixe unique
        counter = 1
        while json_file.exists():
            json_file = (
                self.cache_dir / f"mtgo_complete_data_{timestamp}_{counter}.json"
            )
            counter += 1

        # Sauvegarder en JSON (AJOUT SEULEMENT)
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "scraped_at": datetime.now().isoformat(),
                    "total_items": len(data),
                    "data": data,
                    "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                    "git_rule": "CACHE_NOT_COMMITTED",
                },
                f,
                indent=2,
            )

        self.logger.info(
            f"üíæ {len(data)} √©l√©ments sauvegard√©s dans {json_file} (AJOUT SEULEMENT)"
        )

        # Organiser par date si disponible (AJOUT SEULEMENT)
        dated_data = [item for item in data if item.get("date")]
        if dated_data:
            await self.organize_by_date(dated_data)

    async def organize_by_date(self, dated_data: list):
        """Organise les donn√©es par date (AJOUT SEULEMENT)"""
        for item in dated_data:
            try:
                item_date = datetime.fromisoformat(item["date"])
                year_month_dir = (
                    self.cache_dir / str(item_date.year) / f"{item_date.month:02d}"
                )
                year_month_dir.mkdir(parents=True, exist_ok=True)

                # R√àGLE ABSOLUE : Nom de fichier unique pour √©viter l'√©crasement
                timestamp = datetime.now().strftime("%H%M%S_%f")[
                    :-3
                ]  # Microsecondes pour unicit√©
                filename = f"mtgo_item_{item_date.strftime('%Y%m%d')}_{timestamp}.json"

                # V√©rifier que le fichier n'existe pas d√©j√†
                file_path = year_month_dir / filename
                counter = 1
                while file_path.exists():
                    filename = f"mtgo_item_{item_date.strftime('%Y%m%d')}_{timestamp}_{counter}.json"
                    file_path = year_month_dir / filename
                    counter += 1

                # Sauvegarder (AJOUT SEULEMENT)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            **item,
                            "cache_rule": "PRESERVED_EXISTING_ADDED_NEW_ONLY",
                            "git_rule": "CACHE_NOT_COMMITTED",
                        },
                        f,
                        indent=2,
                    )

            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Erreur organisation par date: {e}")


async def main():
    """Fonction principale"""
    scraper = FixedMTGOScraper()

    print("üéØ SCRAPER MTGO CORRIG√â - SUIVI DES LIENS")
    print("=" * 50)
    print("üö® R√àGLE ABSOLUE : JAMAIS EFFACER LE CACHE EXISTANT")
    print("üìã Seulement AJOUTER de nouvelles donn√©es")
    print("üö´ Aucune suppression, remplacement ou √©crasement autoris√©")
    print("=" * 50)

    await scraper.scrape_mtgo_data()

    print("\n‚úÖ Scraping termin√©!")
    print("‚úÖ R√àGLE RESPECT√âE : Cache existant pr√©serv√©")


if __name__ == "__main__":
    asyncio.run(main())
